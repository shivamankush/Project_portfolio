{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "lkWB88tnqY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6L1LCuGpoak"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-your_api_key\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Load and Parse PDF (Text + OCR)**"
      ],
      "metadata": {
        "id": "gEqfOWuoq3oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: PDF Loading (Text + OCR Hybrid Method)\n",
        "# This step loads both text-based and scanned PDFs.\n",
        "# It first tries simple text extraction (PyPDFLoader),\n",
        "# and if OCR is needed, it uses UnstructuredPDFLoader with hi_res mode."
      ],
      "metadata": {
        "id": "5K9wTY5nq9dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader"
      ],
      "metadata": {
        "id": "fUr_Z-pmslof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\" # change path as needed\n",
        "\n",
        "# Simple text extraction first\n",
        "try:\n",
        "    print(\"Attempting text extraction using PyPDFLoader\")\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Check if text was extracted (PyPDF sometimes returns empty text for scanned PDFs)\n",
        "    total_text = sum(len(doc.page_content.strip()) for doc in documents)\n",
        "    if total_text < 100:  # threshold: if text is too short, likely a scanned PDF\n",
        "        raise ValueError(\"Empty or scanned PDF detected, switching to OCR loader\")\n",
        "\n",
        "    print(f\"Text-based PDF successfully loaded! Extracted {len(documents)} pages.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Text extraction failed ({e}), switching to OCR mode using UnstructuredPDFLoader...\")\n",
        "    loader = UnstructuredPDFLoader(\n",
        "        pdf_path,\n",
        "        mode=\"elements\",       # keeps layout elements (better context)\n",
        "        strategy=\"hi_res\"      # enables image-based OCR parsing internally\n",
        "    )\n",
        "    documents = loader.load()\n",
        "    print(f\"OCR-based PDF successfully loaded using Unstructured loader. Extracted {len(documents)} elements.\")\n",
        "\n",
        "# Preview one document object\n",
        "print(\"\\n--- Sample Extracted Text ---\\n\")\n",
        "print(documents[0].page_content[:800])  # preview first 800 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soOF6YKJs70e",
        "outputId": "688f33d4-b71f-4ea6-cd67-39a9bdd9fd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting text extraction using PyPDFLoader\n",
            "Text-based PDF successfully loaded! Extracted 11 pages.\n",
            "\n",
            "--- Sample Extracted Text ---\n",
            "\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser ∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 — Recursive & Semantic Chunking**"
      ],
      "metadata": {
        "id": "q4_0NjYHQV5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Chunking (Recursive & Semantic)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# (Optional) Semantic chunker if you want deeper contextual splits\n",
        "# from langchain_experimental.text_splitter import SemanticChunker\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Recursive Chunking Setup\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,      # each chunk will have up to 1000 characters\n",
        "    chunk_overlap=100,    # overlap helps preserve context between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "chunks = recursive_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Total chunks created: {len(docs_chunks)}\")\n",
        "print(f\"Example chunk preview:\\n{docs_chunks[0].page_content[:800]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SskRa0pAuHz4",
        "outputId": "8c7339ae-1031-4672-b2af-71cf9d6d2e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 51\n",
            "Example chunk preview:\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser ∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Embedding + Vector Database Setup**"
      ],
      "metadata": {
        "id": "8pHUk6G3UPvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Create Embeddings and Store in ChromaDB\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create ChromaDB vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./chroma_db\"  # Folder to save your vectors\n",
        ")\n",
        "\n",
        "# Persist data (saves locally)\n",
        "vectorstore.persist()\n",
        "\n",
        "print(f\"Stored {len(chunks)} chunks in ChromaDB successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6PltyV0SAtI",
        "outputId": "d17061f1-70f1-4cc5-adee-3e4bd0561909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 51 chunks in ChromaDB successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1561325879.py:17: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5qPzLi4dGAj",
        "outputId": "ef56e5bd-fe4a-4125-f524-73b00e1a7279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Hybrid Retrieval (Semantic + Keyword)\n",
        "\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1. Load Chroma Vector Store (from Step 3) ---\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "semantic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# 2. Create BM25 Retriever (Keyword-based) ---\n",
        "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "bm25_retriever.k = 4\n",
        "\n",
        "# 3. Combine Both Retrievers using EnsembleRetriever ---\n",
        "hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, semantic_retriever],\n",
        "    weights=[0.4, 0.6]  # 40% keyword + 60% semantic importance\n",
        ")\n",
        "\n",
        "# 4️. Initialize LLM for Final Answer Generation ---\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=hybrid_retriever,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "4QD35EN5J1oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Ask Questions from the PDF ---\n",
        "query = \"Explain the main contribution of the Attention Is All You Need paper\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"\\n Query:\", query)\n",
        "print(\"\\n Answer:\\n\", result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppdX9SzWcWe7",
        "outputId": "2e7fb4ab-1333-424b-ddca-922497892815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Query: Explain the main contribution of the Attention Is All You Need paper\n",
            "\n",
            " Answer:\n",
            " The main contribution of the \"Attention Is All You Need\" paper is the introduction of a new network architecture called the Transformer. This architecture relies solely on attention mechanisms without using recurrent or convolutional neural networks. The Transformer model includes a multi-head attention mechanism that allows the model to jointly attend to information from different representation subspaces at different positions. This approach improves the efficiency and performance of sequence transduction models, making them more effective for tasks involving text and potentially other modalities like images, audio, and video.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Ask Questions from the PDF ---\n",
        "query = \"Explain about Tranformer in detail\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"\\n Query:\", query)\n",
        "print(\"\\n Answer:\\n\", result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqVuVlHCgzuC",
        "outputId": "8ce93eb3-b596-46da-e5f8-80f782e7e255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Query: Explain about Tranformer in detail if possible with figure\n",
            "\n",
            " Answer:\n",
            " The Transformer is a neural sequence transduction model that relies entirely on self-attention mechanisms, eliminating the need for recurrence and convolutions. It consists of an encoder-decoder structure where the encoder maps input sequences to continuous representations, and the decoder generates output sequences. The model architecture includes stacked self-attention and point-wise fully connected layers for both the encoder and decoder.\n",
            "\n",
            "In the Transformer architecture, the encoder and decoder are composed of a stack of identical layers. Each layer in the encoder has two sub-layers: a multi-head self-attention mechanism and a point-wise fully connected feed-forward network. The output of each sub-layer is passed through a residual connection and layer normalization. The decoder, in addition to these two sub-layers, inserts a third sub-layer that performs multi-head attention over the output of the encoder stack.\n",
            "\n",
            "The self-attention mechanism in the Transformer allows the model to compute representations of its input and output without using traditional sequence-aligned recurrent neural networks (RNNs) or convolutions. This approach offers advantages in terms of model quality, parallelizability, and training efficiency. The Transformer has shown superior performance in tasks like machine translation, achieving state-of-the-art results with reduced training time.\n",
            "\n",
            "The model also incorporates positional encodings to provide information about the relative or absolute position of tokens in the sequence, compensating for the lack of recurrence and convolution. By injecting positional information into input embeddings, the Transformer can leverage the order of tokens in the sequence effectively.\n",
            "\n",
            "Overall, the Transformer architecture has demonstrated significant improvements in translation quality, training efficiency, and parallelizability compared to traditional models. It represents a novel approach to sequence transduction tasks, relying on self-attention mechanisms for computation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Ask Questions from the PDF ---\n",
        "query = \"Give the summary of the Attention Is All You Need paper\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"\\n Query:\", query)\n",
        "print(\"\\n Answer:\\n\", result[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9ui9NOlhOno",
        "outputId": "37052ffc-902b-44ba-c5ec-989cf132d1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Query: Give the summary of the Attention Is All You Need paper\n",
            "\n",
            " Answer:\n",
            " The \"Attention Is All You Need\" paper introduces a new network architecture called the Transformer, which simplifies sequence transduction models by using attention mechanisms. The paper discusses the concept of attention, including Scaled Dot-Product Attention and Multi-Head Attention. It explains how attention functions map queries and key-value pairs to outputs, emphasizing the importance of jointly attending to information from different representation subspaces. The paper also describes modifications to the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THANKS**"
      ],
      "metadata": {
        "id": "3b9TkuymkHB6"
      }
    }
  ]
}